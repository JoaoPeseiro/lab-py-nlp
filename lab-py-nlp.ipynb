{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf31a39",
   "metadata": {},
   "source": [
    "# **Comprehensive NLP Lab: From Preprocessing to Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5fcd9",
   "metadata": {},
   "source": [
    "In this lab, you will explore a wide range of Natural Language Processing (NLP) techniques, from basic text preprocessing to advanced feature extraction and analysis. By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Tokenize** and preprocess text data.\n",
    "2. Remove **stop words** and **punctuation**.\n",
    "3. Apply **stemming** and **lemmatization**.\n",
    "4. Extract features using **Bag of Words (BoW)** and **TF-IDF**.\n",
    "5. Generate **n-grams** to capture contextual information.\n",
    "6. Evaluate the impact of different preprocessing techniques on text data.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bdb53e",
   "metadata": {},
   "source": [
    "## **1. Setup the Environment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa12605",
   "metadata": {},
   "source": [
    "Before we begin, ensure you have the necessary libraries installed. Run the following cell to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd9b473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\megap\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\megap\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\megap\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\megap\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\megap\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\megap\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\megap\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\megap\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\megap\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk scikit-learn pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839e6cf",
   "metadata": {},
   "source": [
    "Now, import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27aa77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a2544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\megap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\megap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\megap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\megap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\megap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\megap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec67490",
   "metadata": {},
   "source": [
    "## **2. Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6877c3",
   "metadata": {},
   "source": [
    "### **Exercise 1: Tokenization and Stop Word Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956bdaa1",
   "metadata": {},
   "source": [
    "Tokenize the following text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82fcea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
    "x = word_tokenize(text)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26303ab9",
   "metadata": {},
   "source": [
    "Remove stop words and store the result in a variable called `filtered_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09992c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in x if word.lower() not in stop_words]\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14120510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'study', '!', 'involves', 'analyzing', 'understanding', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0b30d",
   "metadata": {},
   "source": [
    "### **Exercise 2: Stemming and Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6949309",
   "metadata": {},
   "source": [
    "Apply stemming and lemmatization to the `filtered_tokens`. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063d90e",
   "metadata": {},
   "source": [
    "Apply stemming and store the result in `stemmed_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2c5d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['natur', 'languag', 'process', '(', 'nlp', ')', 'fascin', 'field', 'studi', '!', 'involv', 'analyz', 'understand', 'human', 'languag', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc333a",
   "metadata": {},
   "source": [
    "Apply lemmatization and store the result in `lemmatized_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b23234",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08f49cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['natural', 'language', 'processing', '(', 'nlp', ')', 'fascinating', 'field', 'study', '!', 'involves', 'analyzing', 'understanding', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20531b9b",
   "metadata": {},
   "source": [
    "## **3. Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbd840",
   "metadata": {},
   "source": [
    "### **Exercise 3: Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7765074",
   "metadata": {},
   "source": [
    "Use the `CountVectorizer` from `scikit-learn` to create a Bag of Words representation of the following corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c86f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love NLP.\",\n",
    "    \"NLP is amazing.\",\n",
    "    \"I enjoy learning new things in NLP.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45334917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = y.fit_transform(corpus)# your code here\n",
    "# Step 1: Initialize the CountVectorizer\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a BoW representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfb3b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      " [[0 0 0 0 0 1 0 1 0]\n",
      " [1 0 0 1 0 0 0 1 0]\n",
      " [0 1 1 0 1 0 1 1 1]]\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBag of Words:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1468\u001b[0m, in \u001b[0;36mCountVectorizer.get_feature_names_out\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_feature_names_out\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \n\u001b[0;32m   1458\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;124;03m        Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m   1470\u001b[0m         [t \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))],\n\u001b[0;32m   1471\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m   1472\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:505\u001b[0m, in \u001b[0;36m_VectorizerMixin._check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_:\n\u001b[1;32m--> 505\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary not fitted or provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words:\\n\", X.toarray())\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d79220",
   "metadata": {},
   "source": [
    "### **Exercise 4: TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55622108",
   "metadata": {},
   "source": [
    "Use the `TfidfVectorizer` from `scikit-learn` to create a TF-IDF representation of the same corpus. Store the result in `X_tfidf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "vectorizer = TfidfVectorizer()# Step 1: Initialize the TfidfVectorizer\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(corpus)# Step 2: Fit and transform the corpus into a TF-IDF representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0df3ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      " [[0.         0.         0.         0.         0.         0.861037\n",
      "  0.         0.50854232 0.        ]\n",
      " [0.65249088 0.         0.         0.65249088 0.         0.\n",
      "  0.         0.38537163 0.        ]\n",
      " [0.         0.43238509 0.43238509 0.         0.43238509 0.\n",
      "  0.43238509 0.2553736  0.43238509]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, X_tfidf\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF:\\n\", X_tfidf.toarray())\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6cee3",
   "metadata": {},
   "source": [
    "### **Exercise 5: N-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9d66d",
   "metadata": {},
   "source": [
    "Generate `bigrams (2-grams)` from the corpus using `CountVectorizer`. Store the result in `X_bigram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9de7b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Step 1: Initialize the CountVectorizer with ngram_range=(2, 2)\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X_bigram = bigram_vectorizer.fit_transform(corpus)# Step 2: Fit and transform the corpus into a bigram representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b210fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:\n",
      " [[0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 1 0]\n",
      " [1 1 0 1 0 1 0 1]]\n",
      "Bigram Vocabulary: ['enjoy learning' 'in nlp' 'is amazing' 'learning new' 'love nlp'\n",
      " 'new things' 'nlp is' 'things in']\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigrams:\\n\", X_bigram.toarray())\n",
    "print(\"Bigram Vocabulary:\", bigram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7896acb",
   "metadata": {},
   "source": [
    "## **4. Advanced Exercise: Custom Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68072a1e",
   "metadata": {},
   "source": [
    "### **Exercise 6: Build a Custom Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb33268",
   "metadata": {},
   "source": [
    "Combine all the preprocessing steps (tokenization, stop word removal, punctuation removal, stemming/lemmatization) into a single function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfff29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def text_preprocessing_pipeline(text):\n",
    "    tokens = word_tokenize(text.lower())# Step 1: Tokenize the text\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words]# Step 2: Remove stop words\n",
    "\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]# Step 3: Remove punctuation\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]# Step 4: Apply lemmatization\n",
    "\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8170a6c",
   "metadata": {},
   "source": [
    "Apply this function to the following text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1777799",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
    "\n",
    "processed_tokens = text_preprocessing_pipeline(text)# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c75b50c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, processed_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_text' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625df20",
   "metadata": {},
   "source": [
    "## **5. Evaluation of Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a666da0",
   "metadata": {},
   "source": [
    "### **Exercise 7: Compare Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae05a37",
   "metadata": {},
   "source": [
    "Compare the results of stemming and lemmatization on the following sentence. Store the results in `stemmed_tokens` and `lemmatized_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e70cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cats are playing with the mice in the garden.\"\n",
    "# your code here\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]# Step 1: Tokenize and preprocess the sentence and store the result in filtered_tokens\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]# Step 2: Apply stemming\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]# Step 3: Apply lemmatization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "125ca6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['cats', 'playing', 'mice', 'garden']\n",
      "Stemmed Tokens: ['cat', 'play', 'mice', 'garden']\n",
      "Lemmatized Tokens: ['cat', 'playing', 'mouse', 'garden']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Tokens:\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c465f",
   "metadata": {},
   "source": [
    "## **6. Real-World Dataset: Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a598d9",
   "metadata": {},
   "source": [
    "### **Exercise 8: Preprocess and Analyze Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056ab80",
   "metadata": {},
   "source": [
    "In this exercise, you will work with a real-world dataset of tweets. The dataset contains 5000 positive and 5000 negative tweets. Your task is to preprocess the tweets and extract features for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1e37f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\megap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data] Error with downloaded zip file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2c60819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43847ae",
   "metadata": {},
   "source": [
    "Load the dataset of positive and negative tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b423ebc",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m positive_tweets \u001b[38;5;241m=\u001b[39m twitter_samples\u001b[38;5;241m.\u001b[39mstrings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive_tweets.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m negative_tweets \u001b[38;5;241m=\u001b[39m twitter_samples\u001b[38;5;241m.\u001b[39mstrings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_tweets.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\site-packages\\nltk\\data.py:551\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    549\u001b[0m modified_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(pieces[:i] \u001b[38;5;241m+\u001b[39m [pieces[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m pieces[i:])\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(modified_name, paths)\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\site-packages\\nltk\\data.py:538\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(p):\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 538\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ZipFilePathPointer(p, zipentry)\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;66;03m# resource not in zipfile\u001b[39;00m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\site-packages\\nltk\\data.py:391\u001b[0m, in \u001b[0;36mZipFilePathPointer.__init__\u001b[1;34m(self, zipfile, entry)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03mCreate a new path pointer pointing at the specified entry\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03min the given zipfile.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03mdoes not contain the specified entry.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(zipfile, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 391\u001b[0m     zipfile \u001b[38;5;241m=\u001b[39m OpenOnDemandZipFile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(zipfile))\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# Check that the entry exists:\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entry:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;66;03m# Normalize the entry string, it should be relative:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\site-packages\\nltk\\data.py:1020\u001b[0m, in \u001b[0;36mOpenOnDemandZipFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReopenableZipFile filename must be a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m zipfile\u001b[38;5;241m.\u001b[39mZipFile\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m==\u001b[39m filename\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\zipfile\\__init__.py:1349\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_RealGetContents()\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1351\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1352\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\megap\\anaconda3\\Lib\\zipfile\\__init__.py:1416\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b8248",
   "metadata": {},
   "source": [
    "Combine them into a single list called ``all_tweets`` and create a corresponding list of labels called `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Combine the datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample tweet\n",
    "print(\"Sample Tweet:\", all_tweets[0])\n",
    "print(\"Label:\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2a04a",
   "metadata": {},
   "source": [
    "### **Exercise 9: Preprocess Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1e984",
   "metadata": {},
   "source": [
    "Apply the custom preprocessing pipeline to the entire dataset of tweets. Store the result in ``preprocessed_tweets``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e8c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply the preprocessing pipeline to all tweets\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeef254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample preprocessed tweet\n",
    "print(\"Preprocessed Tweets Sample:\", preprocessed_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8658daf",
   "metadata": {},
   "source": [
    "### **Exercise 10: Feature Extraction on Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f161c",
   "metadata": {},
   "source": [
    "Extract features from the preprocessed tweets using **Bag of Words** and **TF-IDF**. Store the results in ``X_bow`` and ``X_tfidf``, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee42f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Step 1: Create a Bag of Words representation\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Create a TF-IDF representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912687c",
   "metadata": {},
   "source": [
    "## **7. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ed4ac",
   "metadata": {},
   "source": [
    "In this lab, you explored a wide range of NLP techniques, from basic text preprocessing to advanced feature extraction and analysis. You also worked with a real-world dataset of tweets and applied your knowledge to preprocess and extract features for sentiment analysis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
